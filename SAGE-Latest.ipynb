{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline: FD + class-balanced gradient agreement subset selection\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from functorch import make_functional_with_buffers, vmap, grad\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import PIL.Image as Image\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/trinity/home/a.jha/venvs/jupyter38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from timm.data import create_transform\n",
    "\n",
    "device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- data stats ----------\n",
    "train_raw = CIFAR100('./data100', train=True,  download=True)\n",
    "x = np.concatenate([np.asarray(train_raw[i][0]) for i in range(len(train_raw))])\n",
    "mean = (x.mean((0, 1))/255).tolist()\n",
    "std  = (x.std((0, 1))/255).tolist()\n",
    "\n",
    "# ---------- strong augmentation ----------\n",
    "# RandAugment parameters: N=2 ops, M=9 magnitude (common default)\n",
    "rand_augment = T.RandAugment(num_ops=2, magnitude=9)\n",
    "\n",
    "base_transforms = [\n",
    "    T.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    rand_augment,                       # <-- NEW\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std, inplace=True),\n",
    "    T.RandomErasing(p=0.25, scale=(0.02, 0.2), ratio=(0.3, 3.3),\n",
    "                    value=mean),        # <-- NEW\n",
    "]\n",
    "\n",
    "transform_train = T.Compose(base_transforms)\n",
    "transform_test  = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
    "\n",
    "train_dataset = CIFAR100('./data100', train=True,\n",
    "                         transform=transform_train, download=False)\n",
    "test_dataset  = CIFAR100('./data100', train=False,\n",
    "                         transform=transform_test,  download=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TinyImageNet(Dataset):\n",
    "#     def __init__(self, root, split='train', transform=None):\n",
    "#         self.root_dir = root\n",
    "#         self.split = split\n",
    "#         self.transform = transform\n",
    "#         with open(os.path.join(self.root_dir, 'wnids.txt'), 'r') as f:\n",
    "#             self.classes = f.read().strip().split()\n",
    "#         self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "#         self.class_name = self._get_names()\n",
    "#         self.images = self._load_images()\n",
    "\n",
    "#     def _load_images(self):\n",
    "#         images = []\n",
    "#         if self.split == 'train':\n",
    "#             for cls in self.classes:\n",
    "#                 cls_dir = os.path.join(self.root_dir, self.split, cls, 'images')\n",
    "#                 for image_file in os.listdir(cls_dir):\n",
    "#                     image_path = os.path.join(cls_dir, image_file)\n",
    "#                     images.append((image_path, self.class_to_idx[cls]))\n",
    "#         elif self.split == 'val':\n",
    "#             val_dir = os.path.join(self.root_dir, self.split, 'images')\n",
    "#             image_to_cls = {}\n",
    "#             with open(os.path.join(self.root_dir, self.split, 'val_annotations.txt'), 'r') as f:\n",
    "#                 for line in f.read().strip().split('\\n'):\n",
    "#                     image_to_cls[line.split()[0].strip()] = line.split()[1].strip()\n",
    "#             for image_file in os.listdir(val_dir):\n",
    "#                 image_path = os.path.join(val_dir, image_file)\n",
    "#                 images.append((image_path, self.class_to_idx[image_to_cls[image_file]]))\n",
    "#         return images\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.images)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_path, label = self.images[idx]\n",
    "#         image = Image.open(img_path).convert('RGB')\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         return image, label\n",
    "\n",
    "#     def _get_names(self):\n",
    "#         entity_dict = {}\n",
    "#         with open(os.path.join(self.root_dir, 'words.txt'), 'r') as file:\n",
    "#             for line in file:\n",
    "#                 key, value = line.strip().split('\\t')\n",
    "#                 first = value.strip().split(',')\n",
    "#                 entity_dict[key] = first[0]\n",
    "#         return entity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.transforms as T\n",
    "# from timm.data.mixup import Mixup   # <─ pip install timm if you haven’t\n",
    "# # --------------------------------------------\n",
    "# dirs=\"./tiny-imagenet-200/\"\n",
    "# device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # colour statistics you already use\n",
    "# mean = (0.480, 0.448, 0.397)\n",
    "# std  = (0.276, 0.269, 0.282)\n",
    "\n",
    "# train_tf = T.Compose([\n",
    "#     # PIL-space augmentations\n",
    "#     T.RandomResizedCrop(64, scale=(0.8, 1.0),\n",
    "#                         interpolation=T.InterpolationMode.BICUBIC),\n",
    "#     T.RandomHorizontalFlip(),\n",
    "#     T.RandAugment(num_ops=2, magnitude=9),\n",
    "\n",
    "#     # tensor-space ops\n",
    "#     T.ToTensor(),\n",
    "#     T.RandomErasing(p=0.25, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=mean),\n",
    "#     T.Normalize(mean, std, inplace=True),\n",
    "# ])\n",
    "\n",
    "# # ---------- validation / test ----------\n",
    "# val_tf = T.Compose([\n",
    "#     T.Resize(64, interpolation=T.InterpolationMode.BICUBIC),\n",
    "#     T.CenterCrop(64),\n",
    "#     T.ToTensor(),\n",
    "#     T.Normalize(mean, std, inplace=True),\n",
    "# ])\n",
    "\n",
    "# # 4️⃣  datasets\n",
    "# train_dataset = TinyImageNet(dirs, split='train', transform=train_tf)\n",
    "# test_dataset   = TinyImageNet(dirs, split='val',   transform=val_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirs=\"./imagenet/\"\n",
    "# traindir = os.path.join(dirs, 'train')\n",
    "# valdir = os.path.join(dirs, 'val')\n",
    "\n",
    "# device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# train_dataset = datasets.ImageFolder(\n",
    "#     traindir,\n",
    "#     transforms.Compose([\n",
    "#         transforms.Resize(256),\n",
    "#         transforms.CenterCrop(224),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "    \n",
    "# )\n",
    "\n",
    "# test_dataset = datasets.ImageFolder(\n",
    "#     valdir,\n",
    "#     transforms.Compose([\n",
    "#         transforms.Resize(256),\n",
    "#         transforms.CenterCrop(224),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainloader = torch.utils.data.DataLoader(fullset, batch_size=trn_batch_size,\n",
    "#                                           shuffle=False, pin_memory=True, num_workers=1)\n",
    "\n",
    "# valloader = torch.utils.data.DataLoader(testset, batch_size=val_batch_size,\n",
    "#                                             shuffle=False, pin_memory=True, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def gpu_fd_stream(\n",
    "        A: np.ndarray,\n",
    "        ell: int,\n",
    "        device=None,\n",
    "        batch_size: int = 2048,\n",
    "        dtype: torch.dtype = torch.float16,\n",
    "        svd_dtype: torch.dtype = torch.float32,\n",
    "):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    m, n = A.shape\n",
    "    B = torch.zeros((ell, n), device=device, dtype=dtype)\n",
    "    next_row = 0\n",
    "\n",
    "    for start in range(0, m, batch_size):\n",
    "        batch_gpu = torch.from_numpy(A[start:start + batch_size])\\\n",
    "                 .to(device=device, dtype=dtype, non_blocking=True)\n",
    "\n",
    "\n",
    "        insert = 0\n",
    "        while insert < batch_gpu.shape[0]:\n",
    "            space = ell - next_row\n",
    "            take = min(space, batch_gpu.shape[0] - insert)\n",
    "            B[next_row:next_row + take] = batch_gpu[insert:insert + take]\n",
    "            next_row += take\n",
    "            insert += take\n",
    "\n",
    "            if next_row == ell:                       # overflow → compress\n",
    "                U, s, Vt = torch.linalg.svd(\n",
    "                    B.to(svd_dtype), full_matrices=False\n",
    "                )\n",
    "                delta = s[-1] ** 2\n",
    "                s = torch.sqrt(torch.clamp(s ** 2 - delta, min=0.0))\n",
    "                B = (torch.diag(s).to(dtype) @ Vt.to(dtype))\n",
    "                next_row = (s > 1e-8).sum().item()\n",
    "                if next_row < ell:\n",
    "                    B[next_row:].zero_()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    return B[:next_row].float().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- helper -------------\n",
    "class FDStreamer:\n",
    "    def __init__(self, ell, batch_size=2048, dtype=torch.float16):\n",
    "        self.ell = ell; self.batch_size = batch_size; self.dtype = dtype\n",
    "        self._buf = []      # CPU mini-batches\n",
    "        self._B   = None\n",
    "\n",
    "    def add(self, grad_batch: np.ndarray):\n",
    "        self._buf.append(grad_batch)\n",
    "        if sum(b.shape[0] for b in self._buf) >= self.batch_size:\n",
    "            self._flush()\n",
    "\n",
    "    def _flush(self):\n",
    "        if not self._buf: return\n",
    "        A_cpu = np.vstack(self._buf)\n",
    "        self._buf.clear()\n",
    "        if self._B is None:\n",
    "            self._B = gpu_fd_stream(A_cpu, self.ell,\n",
    "                                    batch_size=self.batch_size,\n",
    "                                    dtype=self.dtype)\n",
    "        else:\n",
    "            A_big = np.vstack([self._B, A_cpu])\n",
    "            self._B = gpu_fd_stream(A_big, self.ell,\n",
    "                                    batch_size=self.batch_size,\n",
    "                                    dtype=self.dtype)\n",
    "\n",
    "    def finalize(self):\n",
    "        self._flush()\n",
    "        return self._B\n",
    "# ------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_per_sample_sketches(\n",
    "#         model, loader, criterion, device, proj_matrix,\n",
    "#         chunk_size: int = 4             # <-- new arg, default 4\n",
    "# ):\n",
    "#     model.eval()\n",
    "#     fmodel, params, buffers = make_functional_with_buffers(model)\n",
    "\n",
    "#     def compute_loss(p, b, x, y):\n",
    "#         out = fmodel(p, b, x.unsqueeze(0)).squeeze(0)\n",
    "#         return criterion(out, y)\n",
    "\n",
    "#     grad_fn = grad(compute_loss)\n",
    "#     grads_all = []\n",
    "\n",
    "#     for x, y in loader:\n",
    "#         x, y = x.to(device), y.to(device)\n",
    "\n",
    "#         # OOM-safe vmap\n",
    "#         grads_batch = vmap(\n",
    "#             grad_fn, in_dims=(None, None, 0, 0),\n",
    "#             chunk_size=chunk_size        # <-----------------\n",
    "#         )(params, buffers, x, y)\n",
    "\n",
    "#         flat = torch.cat(\n",
    "#             [g.reshape(g.shape[0], -1) for g in grads_batch], dim=1\n",
    "#         )\n",
    "#         projected = flat @ proj_matrix.T        # stays on device\n",
    "#         grads_all.append(projected.cpu())       # move to host, free VRAM\n",
    "#         del grads_batch, flat, projected\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#     return torch.cat(grads_all, dim=0)          # (N, ℓ) on CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------- Agreement Selector ---------------------\n",
    "# def select_agreeing_subset(grads: torch.Tensor, subset_size: int):\n",
    "#     grads = grads / (grads.norm(dim=1, keepdim=True) + 1e-8)\n",
    "#     sim_matrix = grads @ grads.T\n",
    "#     agreement = sim_matrix.sum(dim=1)\n",
    "#     top_indices = torch.topk(agreement, subset_size).indices\n",
    "#     return top_indices.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def select_agreeing_subset_fast(\n",
    "#         grads: torch.Tensor,          # (N, ℓ)  on **any device**\n",
    "#         subset_size: int\n",
    "# ) -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     Return indices of the `subset_size` samples whose (ℓ-dim) gradients\n",
    "#     agree most with the centroid.  Memory / time:  O(N·ℓ).\n",
    "#     \"\"\"\n",
    "#     grads = grads / (grads.norm(dim=1, keepdim=True) + 1e-8)   # L2-normalise\n",
    "#     centroid = grads.mean(dim=0)                               # (ℓ,)\n",
    "#     scores = grads @ centroid                                   # (N,)\n",
    "#     top = torch.topk(scores, subset_size, largest=True).indices\n",
    "#     return top.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With diversity Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from __future__ import annotations\n",
    "# from collections import defaultdict\n",
    "# from typing      import List\n",
    "\n",
    "# import torch, tqdm\n",
    "# from torch.utils.data import DataLoader\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# # --------------------------------------------------------------\n",
    "# # utility: project the gradient of ONE sample to ℓ dims\n",
    "# # --------------------------------------------------------------\n",
    "# def _project_single_grad(\n",
    "#         model, x, y,\n",
    "#         criterion,\n",
    "#         proj_matrix: torch.Tensor      # (ℓ, D)  — on same device as model\n",
    "# ) -> torch.Tensor:                    # (ℓ,)\n",
    "#     model.zero_grad(set_to_none=True)           # free old grads\n",
    "#     out   = model(x.unsqueeze(0)).squeeze(0)    # keep batch dim\n",
    "#     loss  = criterion(out, y)\n",
    "#     loss.backward()\n",
    "\n",
    "#     # flatten all parameter grads in registration order\n",
    "#     g_proj = torch.zeros(proj_matrix.size(0), device=proj_matrix.device)\n",
    "#     offset = 0\n",
    "#     for p in model.parameters():\n",
    "#         if p.grad is None:\n",
    "#             continue\n",
    "#         g_flat = p.grad.flatten().to(proj_matrix.dtype)          # (Pi,)\n",
    "#         P_slice = proj_matrix[:, offset: offset + g_flat.numel()]  # (ℓ, Pi)\n",
    "#         g_proj += P_slice @ g_flat                               # accumulate\n",
    "#         offset += g_flat.numel()\n",
    "#     return g_proj\n",
    "\n",
    "\n",
    "# # --------------------------------------------------------------\n",
    "# # main selector (OOM-proof, no functorch)\n",
    "# # --------------------------------------------------------------\n",
    "# def class_balanced_agreeing_subset_fast(\n",
    "#         model,\n",
    "#         dataset,\n",
    "#         num_classes        : int,\n",
    "#         samples_per_class  : int,\n",
    "#         criterion,\n",
    "#         device,\n",
    "#         proj_matrix        : torch.Tensor,   # (ℓ, D)  on same device as model\n",
    "#         batch_size_data    : int = 64,       # images per forward pass\n",
    "#         chunk_size_grad    : int = 4         # #images whose grads we keep at once\n",
    "# ) -> List[int]:\n",
    "#     \"\"\"\n",
    "#     Pick `samples_per_class` images per class with the highest\n",
    "#     gradient-agreement score, **without ever running out of GPU memory**.\n",
    "#     Returns a list of dataset indices.\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     proj_matrix = proj_matrix.to(device)\n",
    "\n",
    "#     loader = DataLoader(dataset,\n",
    "#                         batch_size=batch_size_data,\n",
    "#                         shuffle=False,\n",
    "#                         num_workers=4,\n",
    "#                         pin_memory=True)\n",
    "\n",
    "#     # buckets for low-dim projected grads (stored on CPU)\n",
    "#     grads_per_class   = defaultdict(list)   # list[(Ni_c , ℓ)]\n",
    "#     indices_per_class = defaultdict(list)\n",
    "\n",
    "#     running_idx = 0\n",
    "#     for X, Y in tqdm.tqdm(loader, desc=\"one-pass projected grads\"):\n",
    "#         X = X.to(device, non_blocking=True)\n",
    "#         Y = Y.to(device, non_blocking=True)\n",
    "\n",
    "#         # ----- split current mini-batch into micro-chunks ------------\n",
    "#         B = Y.size(0)\n",
    "#         for s in range(0, B, chunk_size_grad):\n",
    "#             xc = X[s : s + chunk_size_grad]\n",
    "#             yc = Y[s : s + chunk_size_grad]\n",
    "\n",
    "#             # compute projected grad for **each** sample in micro-chunk\n",
    "#             proj_chunk = torch.stack([\n",
    "#                 _project_single_grad(model, xc[i], yc[i],\n",
    "#                                      criterion, proj_matrix)\n",
    "#                 for i in range(yc.size(0))\n",
    "#             ])                                          # (m, ℓ) on GPU\n",
    "#             proj_chunk_cpu = proj_chunk.cpu()           # immediately off-load\n",
    "\n",
    "#             # bucket by class\n",
    "#             for cls in range(num_classes):\n",
    "#                 mask = (yc == cls)\n",
    "#                 if mask.any():\n",
    "#                     grads_per_class[cls].append(proj_chunk_cpu[mask.cpu()])\n",
    "#                     base = running_idx + s\n",
    "#                     idxs = torch.arange(base, base + yc.size(0))[mask.cpu()]\n",
    "#                     indices_per_class[cls].append(idxs)\n",
    "\n",
    "#             del proj_chunk, proj_chunk_cpu\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#         running_idx += B\n",
    "\n",
    "#     # ---------------- agreement scoring -----------------------------\n",
    "#     selected = []\n",
    "#     for cls in range(num_classes):\n",
    "#         if cls not in grads_per_class:\n",
    "#             continue                                    # class absent\n",
    "#         G = torch.cat(grads_per_class[cls], dim=0)      # (Nc , ℓ)\n",
    "#         I = torch.cat(indices_per_class[cls], dim=0)    # (Nc ,)\n",
    "\n",
    "#         # cosine agreement with centroid\n",
    "#         G_norm   = F.normalize(G, dim=1)\n",
    "#         centroid = G_norm.mean(0)\n",
    "#         # scores   = G_norm @ centroid\n",
    "#         # top_k    = torch.topk(scores, samples_per_class).indices\n",
    "#         # -----------------------------------------------------------------\n",
    "#         # inside  for cls in range(num_classes):   (same place as before)\n",
    "#         # -----------------------------------------------------------------\n",
    "#         G_cls = torch.cat(grads_per_class[cls], dim=0)       # (Nc , ℓ)\n",
    "#         I_cls = torch.cat(indices_per_class[cls], dim=0)     # (Nc ,)\n",
    "        \n",
    "#         G_norm   = torch.nn.functional.normalize(G_cls, dim=1)\n",
    "#         centroid = G_norm.mean(dim=0)\n",
    "#         scores   = G_norm @ centroid                         # (Nc,)\n",
    "        \n",
    "#         λ_div = 0.2            # diversity weight  (hyper-parameter)\n",
    "#         k     = samples_per_class\n",
    "\n",
    "#         chosen = []         # local indices inside G_cls\n",
    "#         for _ in range(k):\n",
    "#             if not chosen:                              # ---------- first pick\n",
    "#                 idx = scores.argmax()                   # pure agreement\n",
    "#             else:                                       # ---------- later picks\n",
    "#                 idxs_t   = torch.tensor(chosen, device=G_norm.device)\n",
    "#                 S        = G_norm[idxs_t]               # (p, ℓ)   p = len(chosen)\n",
    "#                 if S.dim() == 1:                        # happens when p == 1\n",
    "#                     S = S.unsqueeze(0)                  # make it (1, ℓ)\n",
    "        \n",
    "#                 sim_to_S  = (G_norm @ S.T).max(dim=1).values\n",
    "#                 adj_score = scores - λ_div * sim_to_S\n",
    "#                 idx       = adj_score.argmax()\n",
    "        \n",
    "#             chosen.append(idx.item())  \n",
    "        \n",
    "#         selected.extend(I_cls[torch.tensor(chosen)].tolist())\n",
    "#         # -----------------------------------------------------------------\n",
    "\n",
    "#         # selected.extend(I[top_k].tolist())\n",
    "\n",
    "#     return selected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Diversity penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from collections import defaultdict\n",
    "from typing      import List\n",
    "\n",
    "import torch, tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# utility: project the gradient of ONE sample to ℓ dims\n",
    "# --------------------------------------------------------------\n",
    "def _project_single_grad(\n",
    "        model, x, y,\n",
    "        criterion,\n",
    "        proj_matrix: torch.Tensor      # (ℓ, D)  — on same device as model\n",
    ") -> torch.Tensor:                    # (ℓ,)\n",
    "    model.zero_grad(set_to_none=True)           # free old grads\n",
    "    out   = model(x.unsqueeze(0)).squeeze(0)    # keep batch dim\n",
    "    loss  = criterion(out, y)\n",
    "    loss.backward()\n",
    "\n",
    "    # flatten all parameter grads in registration order\n",
    "    g_proj = torch.zeros(proj_matrix.size(0), device=proj_matrix.device)\n",
    "    offset = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is None:\n",
    "            continue\n",
    "        g_flat = p.grad.flatten().to(proj_matrix.dtype)          # (Pi,)\n",
    "        P_slice = proj_matrix[:, offset: offset + g_flat.numel()]  # (ℓ, Pi)\n",
    "        g_proj += P_slice @ g_flat                               # accumulate\n",
    "        offset += g_flat.numel()\n",
    "    return g_proj\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# main selector (OOM-proof, no functorch)\n",
    "# --------------------------------------------------------------\n",
    "def class_balanced_agreeing_subset_fast(\n",
    "        model,\n",
    "        dataset,\n",
    "        num_classes        : int,\n",
    "        samples_per_class  : int,\n",
    "        criterion,\n",
    "        device,\n",
    "        proj_matrix        : torch.Tensor,   # (ℓ, D)  on same device as model\n",
    "        batch_size_data    : int = 64,       # images per forward pass\n",
    "        chunk_size_grad    : int = 4         # #images whose grads we keep at once\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Pick `samples_per_class` images per class with the highest\n",
    "    gradient-agreement score, **without ever running out of GPU memory**.\n",
    "    Returns a list of dataset indices.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    proj_matrix = proj_matrix.to(device)\n",
    "\n",
    "    loader = DataLoader(dataset,\n",
    "                        batch_size=batch_size_data,\n",
    "                        shuffle=False,\n",
    "                        num_workers=4,\n",
    "                        pin_memory=True)\n",
    "\n",
    "    # buckets for low-dim projected grads (stored on CPU)\n",
    "    grads_per_class   = defaultdict(list)   # list[(Ni_c , ℓ)]\n",
    "    indices_per_class = defaultdict(list)\n",
    "\n",
    "    running_idx = 0\n",
    "    for X, Y in tqdm.tqdm(loader, desc=\"one-pass projected grads\"):\n",
    "        X = X.to(device, non_blocking=True)\n",
    "        Y = Y.to(device, non_blocking=True)\n",
    "\n",
    "        # ----- split current mini-batch into micro-chunks ------------\n",
    "        B = Y.size(0)\n",
    "        for s in range(0, B, chunk_size_grad):\n",
    "            xc = X[s : s + chunk_size_grad]\n",
    "            yc = Y[s : s + chunk_size_grad]\n",
    "\n",
    "            # compute projected grad for **each** sample in micro-chunk\n",
    "            proj_chunk = torch.stack([\n",
    "                _project_single_grad(model, xc[i], yc[i],\n",
    "                                     criterion, proj_matrix)\n",
    "                for i in range(yc.size(0))\n",
    "            ])                                          # (m, ℓ) on GPU\n",
    "            proj_chunk_cpu = proj_chunk.cpu()           # immediately off-load\n",
    "\n",
    "            # bucket by class\n",
    "            for cls in range(num_classes):\n",
    "                mask = (yc == cls)\n",
    "                if mask.any():\n",
    "                    grads_per_class[cls].append(proj_chunk_cpu[mask.cpu()])\n",
    "                    base = running_idx + s\n",
    "                    idxs = torch.arange(base, base + yc.size(0))[mask.cpu()]\n",
    "                    indices_per_class[cls].append(idxs)\n",
    "\n",
    "            del proj_chunk, proj_chunk_cpu\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        running_idx += B\n",
    "\n",
    "    # ---------------- agreement scoring -----------------------------\n",
    "    selected = []\n",
    "    for cls in range(num_classes):\n",
    "        if cls not in grads_per_class:\n",
    "            continue                                    # class absent\n",
    "        G = torch.cat(grads_per_class[cls], dim=0)      # (Nc , ℓ)\n",
    "        I = torch.cat(indices_per_class[cls], dim=0)    # (Nc ,)\n",
    "\n",
    "        # cosine agreement with centroid\n",
    "        G_norm   = F.normalize(G, dim=1)\n",
    "        centroid = G_norm.mean(0)\n",
    "        scores   = G_norm @ centroid\n",
    "        top_k    = torch.topk(scores, samples_per_class).indices\n",
    "        selected.extend(I[top_k].tolist())\n",
    "\n",
    "    return selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import annotations\n",
    "# from typing import List\n",
    "# import torch, tqdm\n",
    "# from torch.utils.data import DataLoader\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# # ------------------------------------------------------------------\n",
    "# # project ONE sample’s gradient into ℓ-dim space (unchanged)\n",
    "# # ------------------------------------------------------------------\n",
    "# def _project_single_grad(\n",
    "#     model, x, y,\n",
    "#     criterion,\n",
    "#     proj_matrix: torch.Tensor        # (ℓ, D) on same device as model\n",
    "# ) -> torch.Tensor:                   # (ℓ,)\n",
    "#     model.zero_grad(set_to_none=True)\n",
    "#     loss = criterion(model(x.unsqueeze(0)).squeeze(0), y)\n",
    "#     loss.backward()\n",
    "\n",
    "#     g_proj = torch.zeros(proj_matrix.size(0), device=proj_matrix.device)\n",
    "#     offset = 0\n",
    "#     for p in model.parameters():\n",
    "#         if p.grad is None:\n",
    "#             continue\n",
    "#         g_flat  = p.grad.flatten().to(proj_matrix.dtype)\n",
    "#         P_slice = proj_matrix[:, offset : offset + g_flat.numel()]\n",
    "#         g_proj += P_slice @ g_flat\n",
    "#         offset += g_flat.numel()\n",
    "#     return g_proj\n",
    "\n",
    "\n",
    "# # ------------------------------------------------------------------\n",
    "# # GLOBAL agreeing-subset selector  (no per-class buckets)\n",
    "# # ------------------------------------------------------------------\n",
    "# def agreeing_subset_fast(\n",
    "#     model,\n",
    "#     dataset,\n",
    "#     subset_size        : int,           # total samples to keep\n",
    "#     criterion,\n",
    "#     device,\n",
    "#     proj_matrix        : torch.Tensor,  # (ℓ, D)  on same device\n",
    "#     batch_size_data    : int = 64,\n",
    "#     chunk_size_grad    : int = 4\n",
    "# ) -> List[int]:\n",
    "#     \"\"\"\n",
    "#     Pick `subset_size` images whose projected gradients have the highest\n",
    "#     agreement with the global centroid.  OOM-safe and single-pass.\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     proj_matrix = proj_matrix.to(device)\n",
    "\n",
    "#     loader = DataLoader(\n",
    "#         dataset,\n",
    "#         batch_size=batch_size_data,\n",
    "#         shuffle=False,\n",
    "#         num_workers=4,\n",
    "#         pin_memory=True,\n",
    "#     )\n",
    "\n",
    "#     all_grads   = []            # list[(m, ℓ)] on CPU\n",
    "#     all_indices = []            # matching dataset indices (tensor CPU)\n",
    "\n",
    "#     running_idx = 0\n",
    "#     for X, Y in tqdm.tqdm(loader, desc=\"one-pass projected grads\"):\n",
    "#         X = X.to(device, non_blocking=True)\n",
    "#         Y = Y.to(device, non_blocking=True)\n",
    "\n",
    "#         B = Y.size(0)\n",
    "#         for s in range(0, B, chunk_size_grad):\n",
    "#             xc = X[s : s + chunk_size_grad]\n",
    "#             yc = Y[s : s + chunk_size_grad]\n",
    "\n",
    "#             proj_chunk = torch.stack([\n",
    "#                 _project_single_grad(model, xc[i], yc[i],\n",
    "#                                      criterion, proj_matrix)\n",
    "#                 for i in range(yc.size(0))\n",
    "#             ])                                       # (m, ℓ) GPU\n",
    "#             all_grads.append(proj_chunk.cpu())       # off-load\n",
    "#             base = running_idx + s\n",
    "#             all_indices.append(\n",
    "#                 torch.arange(base, base + yc.size(0)).cpu()\n",
    "#             )\n",
    "\n",
    "#             del proj_chunk\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#         running_idx += B\n",
    "\n",
    "#     # ---------------- agreement scoring ---------------------------\n",
    "#     G = torch.cat(all_grads,   dim=0)                # (N, ℓ)  CPU\n",
    "#     I = torch.cat(all_indices, dim=0)                # (N,)    CPU\n",
    "\n",
    "#     G_norm   = F.normalize(G, dim=1)\n",
    "#     centroid = G_norm.mean(0)\n",
    "#     scores   = G_norm @ centroid                     # (N,)\n",
    "\n",
    "#     top = torch.topk(scores, subset_size).indices\n",
    "#     return I[top].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- Train/Eval ---------------------\n",
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        correct += (outputs.argmax(1) == targets).sum().item()\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            correct += (outputs.argmax(1) == targets).sum().item()\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------- Main ---------------------\n",
    "subset_size = 2500\n",
    "num_classes = 100\n",
    "samples_per_class = subset_size // num_classes\n",
    "# model_for_grad = resnet18(num_classes=100).to(device)\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "weights = ResNet18_Weights.IMAGENET1K_V1      # or V2\n",
    "model_for_grad = resnet18(weights=weights)\n",
    "\n",
    "# 2. replace the classification head for your 100-class task\n",
    "in_features = model_for_grad.fc.in_features\n",
    "model_for_grad.fc = torch.nn.Linear(in_features, num_classes)\n",
    "\n",
    "# 3. (optional) freeze lower layers if you only care about the head\n",
    "for param in model_for_grad.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_for_grad.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model_for_grad = model_for_grad.to(device)\n",
    "# model_for_grad = convert_bn_to_gn(model_for_grad).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# model_for_grad = SimpleMLP().to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size_data = 64            # image batch size (tune to your GPU RAM)\n",
    "train_loader   = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size_data,\n",
    "    shuffle=False,               # no need to shuffle for the sketch\n",
    "    num_workers=4,               # >0 = load data in parallel\n",
    "    pin_memory=True,             # faster host→device copies\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def per_sample_grads_slow(model, x, y):\n",
    "    \"\"\"\n",
    "    Returns a (B, D) NumPy array: one gradient vector per sample.\n",
    "    Works with any model and loss.\n",
    "    \"\"\"\n",
    "    model.eval() \n",
    "    B = x.size(0)\n",
    "    grads = []\n",
    "\n",
    "    for i in range(B):\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        logits = model(x[i : i + 1])          # keep batch dim\n",
    "        loss   = F.cross_entropy(logits, y[i : i + 1])\n",
    "        loss.backward()\n",
    "\n",
    "        # flatten all parameter grads into one long vector\n",
    "        g = torch.cat([\n",
    "            p.grad.flatten() for p in model.parameters() if p.requires_grad\n",
    "        ]).cpu().numpy()\n",
    "        grads.append(g)\n",
    "        torch.cuda.empty_cache()  \n",
    "    return np.stack(grads, axis=0)            # (B, D)\n",
    "\n",
    "# def per_sample_proj_grad(\n",
    "#         model, x, y, criterion,\n",
    "#         proj_matrix: torch.Tensor | None = None,\n",
    "#         dtype: torch.dtype = torch.float16,          # keeps slices small\n",
    "# ) -> torch.Tensor:\n",
    "#     \"\"\"\n",
    "#     • If `proj_matrix` is given → returns (ℓ,) projected gradient.\n",
    "#     • If `proj_matrix is None` → returns the *full* flattened gradient (D,).\n",
    "#     Both results live on **CPU**; only tiny temporaries sit on GPU.\n",
    "#     \"\"\"\n",
    "#     model.zero_grad(set_to_none=True)\n",
    "#     out  = model(x.unsqueeze(0)).squeeze(0)\n",
    "#     loss = criterion(out, y)\n",
    "#     loss.backward()\n",
    "\n",
    "#     # flatten all parameter grads\n",
    "#     g_flat = torch.cat([p.grad.flatten().to(dtype)\n",
    "#                         for p in model.parameters() if p.grad is not None])\n",
    "\n",
    "#     if proj_matrix is None:                # bootstrap mode\n",
    "#         return g_flat.cpu()                # (D,)\n",
    "#     else:                                  # normal projected mode\n",
    "#         proj_matrix = proj_matrix.to(dtype)\n",
    "#         g_proj = (proj_matrix @ g_flat).cpu()   # (ℓ,)\n",
    "#         return g_proj\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = 256                      # rows of the projection / sketch\n",
    "# # fd = FDStreamer(d)         # create once\n",
    "# # criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "# fd = FDStreamer(\n",
    "#         d,\n",
    "#         batch_size = 32,           # ①  never accumulate >2 rows\n",
    "#         dtype       = torch.float16, #    rows are 2× smaller   \n",
    "# )  \n",
    "# # model_for_grad.to(device)\n",
    "# # model_for_grad.eval()        # (no dropout / BN stats)\n",
    "\n",
    "\n",
    "\n",
    "# # --------- configurable quota -----------------------------------\n",
    "# quota_per_class = None      # None → use *all* samples\n",
    "# # quota_per_class = 10      # e.g. set to 10 to keep ≤10 rows / class\n",
    "# # ---------------------------------------------------------------\n",
    "\n",
    "# seen_count = [0] * num_classes        # per-class counter\n",
    "\n",
    "# for x, y in tqdm.tqdm(train_loader, desc=\"stream rows into sketch\"):\n",
    "#     if quota_per_class is None:\n",
    "#         # --- no quota: take the whole batch --------------------\n",
    "#         x_sub, y_sub = x.to(device), y.to(device)\n",
    "\n",
    "#     else:\n",
    "#         # --- quota active: mask out classes already full -------\n",
    "#         need_mask = torch.tensor(\n",
    "#             [seen_count[c.item()] < quota_per_class for c in y]\n",
    "#         )\n",
    "#         if not need_mask.any():          # batch gives nothing new\n",
    "#             continue\n",
    "\n",
    "#         x_sub, y_sub = x[need_mask].to(device), y[need_mask].to(device)\n",
    "\n",
    "#     # --------- compute projected gradients ----------------------\n",
    "#     grads = per_sample_grads_slow(model_for_grad, x_sub, y_sub)\n",
    "#     fd.add(grads)                       # stream into FD sketch\n",
    "\n",
    "#     # --------- update counters ----------------------------------\n",
    "#     if quota_per_class is not None:\n",
    "#         for c in y_sub.cpu():\n",
    "#             seen_count[c] += 1\n",
    "#         if min(seen_count) >= quota_per_class:\n",
    "#             break                       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B_sketch    = fd.finalize()          # shape (d, D)  -- NumPy CPU\n",
    "# proj_matrix = torch.from_numpy(B_sketch)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# final_subset_idx = class_balanced_agreeing_subset_fast(\n",
    "#     model_for_grad,\n",
    "#     train_dataset,\n",
    "#     num_classes=num_classes,\n",
    "#     samples_per_class=samples_per_class,\n",
    "#     criterion=criterion,\n",
    "#     device=device,\n",
    "#     proj_matrix=proj_matrix,\n",
    "#     batch_size_data=128,           # data loader batch\n",
    "#     chunk_size_grad=64             # NEW — vmap micro-batch\n",
    "# )\n",
    "\n",
    "# final_subset = Subset(train_dataset, final_subset_idx)\n",
    "# final_loader = DataLoader(final_subset, batch_size=64, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from timm.data.mixup import Mixup\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from model_factory import create_model\n",
    "\n",
    "num_classes = 100\n",
    "cut_mix = True   # Set to True to enable Mixup/CutMix\n",
    "\n",
    "model = create_model(\n",
    "    \"resnext\",    # alias → cifar_resnext29_32x4d\n",
    "    num_classes=num_classes,\n",
    "    pretrained=False\n",
    ").to(device)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  Augment helpers\n",
    "# ------------------------------------------------------------------\n",
    "if cut_mix:\n",
    "    mixup_fn = Mixup(\n",
    "        mixup_alpha   = 0.2,\n",
    "        cutmix_alpha  = 1.0,\n",
    "        prob          = 1.0,      # always apply either MixUp or CutMix\n",
    "        switch_prob   = 0.5,      # 50-50 split\n",
    "        label_smoothing = 0.1,\n",
    "        num_classes   = num_classes,\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')   # per-sample for MixUp/CutMix\n",
    "else:\n",
    "    mixup_fn = None\n",
    "    criterion = nn.CrossEntropyLoss()   # standard\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  Optimiser & cosine schedule\n",
    "# ------------------------------------------------------------------\n",
    "optimizer  = optim.SGD(model.parameters(),\n",
    "                       lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "epochs     = 200\n",
    "scheduler  = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "scaler     = torch.cuda.amp.GradScaler()  # AMP\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  Training / eval loops\n",
    "# ------------------------------------------------------------------\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    loss_sum, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        if cut_mix:\n",
    "            x, y = mixup_fn(x, y)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits = model(x)\n",
    "            # Mixup/CutMix: criterion returns (N,) vector, else scalar\n",
    "            if cut_mix:\n",
    "                loss = criterion(logits, y).mean()\n",
    "            else:\n",
    "                loss = criterion(logits, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        total    += x.size(0)\n",
    "        with torch.no_grad():\n",
    "            if cut_mix:\n",
    "                # y is soft (one-hot or mixup), accuracy by argmax of both\n",
    "                preds = logits.argmax(1)\n",
    "                targets = y.argmax(1)\n",
    "                correct += (preds == targets).sum().item()\n",
    "            else:\n",
    "                preds = logits.argmax(1)\n",
    "                correct += (preds == y).sum().item()\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ce       = nn.CrossEntropyLoss()\n",
    "    loss_sum, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss_sum += ce(logits, y).item() * x.size(0)\n",
    "        total    += x.size(0)\n",
    "        correct  += (logits.argmax(1) == y).sum().item()\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5.  Train                                                                   \n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    \n",
    "# for epoch in range(1, epochs + 1):\n",
    "#     train_loss, train_acc = train_one_epoch(model, final_loader)\n",
    "#     val_loss,   val_acc   = evaluate(model, test_loader)\n",
    "\n",
    "#     scheduler.step()            # cosine annealing step\n",
    "\n",
    "#     lr_now = scheduler.get_last_lr()[0]\n",
    "#     print(f\"[{epoch:3d}/{epochs}] lr {lr_now:7.5f} | \"\n",
    "#           f\"train {train_acc*100:5.2f}% | val {val_acc*100:5.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sketch(model, loader, proj_matrix=None, d=256,\n",
    "                 batch_size_fd=32, device=\"cuda\"):\n",
    "    fd = FDStreamer(d, batch_size=batch_size_fd, dtype=torch.float16)\n",
    "    for xb, yb in tqdm.tqdm(loader, desc=\"stream rows into sketch\"):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        rows = per_sample_grads_slow(model, xb, yb)\n",
    "        fd.add(rows)\n",
    "    return torch.from_numpy(fd.finalize())        # (d, D)  on CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⇢ Selecting subset at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stream rows into sketch: 100%|████████████████| 250/250 [19:29<00:00,  4.68s/it]\n",
      "one-pass projected grads: 100%|███████████████| 250/250 [02:31<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   new subset size: 2500\n",
      "[  1/200] lr 0.09999 | train  0.92% | val  1.38%\n",
      "[  2/200] lr 0.09998 | train  1.36% | val  1.01%\n",
      "[  3/200] lr 0.09994 | train  1.72% | val  1.19%\n",
      "[  4/200] lr 0.09990 | train  1.56% | val  2.05%\n",
      "[  5/200] lr 0.09985 | train  3.20% | val  3.79%\n",
      "[  6/200] lr 0.09978 | train  4.44% | val  4.91%\n",
      "[  7/200] lr 0.09970 | train  6.20% | val  5.86%\n",
      "[  8/200] lr 0.09961 | train  6.72% | val  6.07%\n",
      "[  9/200] lr 0.09950 | train  8.00% | val  7.27%\n",
      "[ 10/200] lr 0.09938 | train  6.24% | val  7.32%\n",
      "[ 11/200] lr 0.09926 | train  8.88% | val  7.97%\n",
      "[ 12/200] lr 0.09911 | train  8.40% | val  7.86%\n",
      "[ 13/200] lr 0.09896 | train 10.24% | val  8.45%\n",
      "[ 14/200] lr 0.09880 | train  9.04% | val  8.97%\n",
      "[ 15/200] lr 0.09862 | train 11.28% | val  8.91%\n",
      "[ 16/200] lr 0.09843 | train 10.68% | val  9.42%\n",
      "[ 17/200] lr 0.09823 | train 10.16% | val  9.79%\n",
      "[ 18/200] lr 0.09801 | train 11.84% | val  9.56%\n",
      "[ 19/200] lr 0.09779 | train 11.28% | val  9.60%\n",
      "[ 20/200] lr 0.09755 | train 12.92% | val 10.09%\n",
      "[ 21/200] lr 0.09730 | train 12.80% | val 10.28%\n",
      "[ 22/200] lr 0.09704 | train 13.92% | val 10.72%\n",
      "[ 23/200] lr 0.09677 | train 14.00% | val 11.08%\n",
      "[ 24/200] lr 0.09649 | train 13.04% | val 10.97%\n",
      "[ 25/200] lr 0.09619 | train 14.72% | val 11.35%\n",
      "[ 26/200] lr 0.09589 | train 16.76% | val 10.87%\n",
      "[ 27/200] lr 0.09557 | train 15.52% | val 12.40%\n",
      "[ 28/200] lr 0.09524 | train 16.28% | val 11.53%\n",
      "[ 29/200] lr 0.09490 | train 15.16% | val 12.97%\n",
      "[ 30/200] lr 0.09455 | train 17.16% | val 12.94%\n",
      "[ 31/200] lr 0.09419 | train 18.20% | val 12.91%\n",
      "[ 32/200] lr 0.09382 | train 18.72% | val 13.86%\n",
      "[ 33/200] lr 0.09343 | train 16.52% | val 13.67%\n",
      "[ 34/200] lr 0.09304 | train 19.64% | val 15.14%\n",
      "[ 35/200] lr 0.09263 | train 19.56% | val 14.02%\n",
      "[ 36/200] lr 0.09222 | train 21.44% | val 14.60%\n",
      "[ 37/200] lr 0.09179 | train 21.72% | val 15.54%\n",
      "[ 38/200] lr 0.09135 | train 23.36% | val 15.66%\n",
      "[ 39/200] lr 0.09091 | train 20.00% | val 16.28%\n",
      "[ 40/200] lr 0.09045 | train 21.32% | val 16.92%\n",
      "[ 41/200] lr 0.08998 | train 18.08% | val 16.43%\n",
      "[ 42/200] lr 0.08951 | train 27.08% | val 16.95%\n",
      "[ 43/200] lr 0.08902 | train 20.56% | val 16.82%\n",
      "[ 44/200] lr 0.08853 | train 25.36% | val 16.61%\n",
      "[ 45/200] lr 0.08802 | train 27.64% | val 17.70%\n",
      "[ 46/200] lr 0.08751 | train 26.00% | val 16.81%\n",
      "[ 47/200] lr 0.08698 | train 25.16% | val 19.67%\n",
      "[ 48/200] lr 0.08645 | train 28.68% | val 19.65%\n",
      "[ 49/200] lr 0.08591 | train 27.08% | val 18.07%\n",
      "[ 50/200] lr 0.08536 | train 31.36% | val 18.71%\n",
      "[ 51/200] lr 0.08480 | train 29.72% | val 20.68%\n",
      "[ 52/200] lr 0.08423 | train 29.36% | val 20.08%\n",
      "[ 53/200] lr 0.08365 | train 27.56% | val 20.48%\n",
      "[ 54/200] lr 0.08307 | train 31.52% | val 20.34%\n",
      "[ 55/200] lr 0.08247 | train 31.04% | val 21.38%\n",
      "[ 56/200] lr 0.08187 | train 34.20% | val 21.35%\n",
      "[ 57/200] lr 0.08126 | train 36.08% | val 22.43%\n",
      "[ 58/200] lr 0.08065 | train 30.36% | val 22.43%\n",
      "[ 59/200] lr 0.08002 | train 39.72% | val 23.40%\n",
      "[ 60/200] lr 0.07939 | train 37.20% | val 21.81%\n",
      "[ 61/200] lr 0.07875 | train 32.48% | val 22.01%\n",
      "[ 62/200] lr 0.07810 | train 39.68% | val 20.96%\n",
      "[ 63/200] lr 0.07745 | train 30.80% | val 23.76%\n",
      "[ 64/200] lr 0.07679 | train 39.48% | val 24.03%\n",
      "[ 65/200] lr 0.07612 | train 37.60% | val 23.27%\n",
      "[ 66/200] lr 0.07545 | train 39.68% | val 24.65%\n",
      "[ 67/200] lr 0.07477 | train 40.96% | val 23.27%\n",
      "[ 68/200] lr 0.07409 | train 41.76% | val 24.91%\n",
      "[ 69/200] lr 0.07340 | train 42.08% | val 24.04%\n",
      "[ 70/200] lr 0.07270 | train 45.68% | val 25.96%\n",
      "[ 71/200] lr 0.07200 | train 45.08% | val 24.93%\n",
      "[ 72/200] lr 0.07129 | train 48.20% | val 25.65%\n",
      "[ 73/200] lr 0.07058 | train 46.64% | val 23.84%\n",
      "[ 74/200] lr 0.06986 | train 44.76% | val 25.74%\n",
      "[ 75/200] lr 0.06913 | train 34.68% | val 24.34%\n",
      "[ 76/200] lr 0.06841 | train 46.28% | val 24.43%\n",
      "[ 77/200] lr 0.06767 | train 50.56% | val 25.78%\n",
      "[ 78/200] lr 0.06694 | train 52.04% | val 26.96%\n",
      "[ 79/200] lr 0.06620 | train 57.16% | val 26.49%\n",
      "[ 80/200] lr 0.06545 | train 53.44% | val 27.56%\n",
      "[ 81/200] lr 0.06470 | train 49.36% | val 27.90%\n",
      "[ 82/200] lr 0.06395 | train 52.96% | val 27.39%\n",
      "[ 83/200] lr 0.06319 | train 45.00% | val 28.53%\n",
      "[ 84/200] lr 0.06243 | train 54.52% | val 27.75%\n",
      "[ 85/200] lr 0.06167 | train 49.64% | val 28.76%\n",
      "[ 86/200] lr 0.06091 | train 54.72% | val 26.99%\n",
      "[ 87/200] lr 0.06014 | train 45.36% | val 26.15%\n",
      "[ 88/200] lr 0.05937 | train 56.12% | val 27.33%\n",
      "[ 89/200] lr 0.05860 | train 48.40% | val 27.49%\n",
      "[ 90/200] lr 0.05782 | train 56.76% | val 27.54%\n",
      "[ 91/200] lr 0.05705 | train 53.20% | val 29.12%\n",
      "[ 92/200] lr 0.05627 | train 53.64% | val 28.77%\n",
      "[ 93/200] lr 0.05549 | train 60.52% | val 28.22%\n",
      "[ 94/200] lr 0.05471 | train 47.96% | val 29.51%\n",
      "[ 95/200] lr 0.05392 | train 60.12% | val 29.55%\n",
      "[ 96/200] lr 0.05314 | train 54.76% | val 29.23%\n",
      "[ 97/200] lr 0.05236 | train 59.48% | val 28.60%\n",
      "[ 98/200] lr 0.05157 | train 56.40% | val 28.63%\n",
      "[ 99/200] lr 0.05079 | train 59.36% | val 29.56%\n",
      "[100/200] lr 0.05000 | train 62.32% | val 30.00%\n",
      "[101/200] lr 0.04921 | train 56.48% | val 30.13%\n",
      "[102/200] lr 0.04843 | train 67.64% | val 31.33%\n",
      "[103/200] lr 0.04764 | train 59.88% | val 31.32%\n",
      "[104/200] lr 0.04686 | train 64.64% | val 30.85%\n",
      "[105/200] lr 0.04608 | train 71.84% | val 31.62%\n",
      "[106/200] lr 0.04529 | train 70.80% | val 31.84%\n",
      "[107/200] lr 0.04451 | train 72.00% | val 30.78%\n",
      "[108/200] lr 0.04373 | train 60.84% | val 31.31%\n",
      "[109/200] lr 0.04295 | train 57.56% | val 31.82%\n",
      "[110/200] lr 0.04218 | train 57.52% | val 31.31%\n",
      "[111/200] lr 0.04140 | train 67.24% | val 31.88%\n",
      "[112/200] lr 0.04063 | train 61.56% | val 31.76%\n",
      "[113/200] lr 0.03986 | train 64.72% | val 32.89%\n",
      "[114/200] lr 0.03909 | train 74.64% | val 32.90%\n",
      "[115/200] lr 0.03833 | train 68.44% | val 33.31%\n",
      "[116/200] lr 0.03757 | train 78.08% | val 32.04%\n",
      "[117/200] lr 0.03681 | train 60.56% | val 31.93%\n",
      "[118/200] lr 0.03605 | train 69.64% | val 32.46%\n",
      "[119/200] lr 0.03530 | train 66.12% | val 32.22%\n",
      "[120/200] lr 0.03455 | train 72.52% | val 33.32%\n",
      "[121/200] lr 0.03380 | train 60.16% | val 32.14%\n",
      "[122/200] lr 0.03306 | train 65.16% | val 32.20%\n",
      "[123/200] lr 0.03233 | train 77.84% | val 35.01%\n",
      "[124/200] lr 0.03159 | train 80.12% | val 33.20%\n",
      "[125/200] lr 0.03087 | train 71.92% | val 34.26%\n",
      "[126/200] lr 0.03014 | train 76.24% | val 33.27%\n",
      "[127/200] lr 0.02942 | train 62.56% | val 32.71%\n",
      "[128/200] lr 0.02871 | train 72.80% | val 34.10%\n",
      "[129/200] lr 0.02800 | train 61.04% | val 32.74%\n",
      "[130/200] lr 0.02730 | train 71.48% | val 34.64%\n",
      "[131/200] lr 0.02660 | train 77.48% | val 34.95%\n",
      "[132/200] lr 0.02591 | train 78.80% | val 35.23%\n",
      "[133/200] lr 0.02523 | train 77.96% | val 35.22%\n",
      "[134/200] lr 0.02455 | train 79.12% | val 34.46%\n",
      "[135/200] lr 0.02388 | train 78.60% | val 34.77%\n",
      "[136/200] lr 0.02321 | train 72.48% | val 36.08%\n",
      "[137/200] lr 0.02255 | train 79.00% | val 34.35%\n",
      "[138/200] lr 0.02190 | train 78.52% | val 35.11%\n",
      "[139/200] lr 0.02125 | train 82.20% | val 34.83%\n",
      "[140/200] lr 0.02061 | train 85.16% | val 35.44%\n",
      "[141/200] lr 0.01998 | train 75.76% | val 34.43%\n",
      "[142/200] lr 0.01935 | train 73.56% | val 35.73%\n",
      "[143/200] lr 0.01874 | train 75.36% | val 35.38%\n",
      "[144/200] lr 0.01813 | train 68.48% | val 35.35%\n",
      "[145/200] lr 0.01753 | train 77.72% | val 35.24%\n",
      "[146/200] lr 0.01693 | train 70.00% | val 36.38%\n",
      "[147/200] lr 0.01635 | train 84.48% | val 36.21%\n",
      "[148/200] lr 0.01577 | train 83.84% | val 34.86%\n",
      "[149/200] lr 0.01520 | train 67.88% | val 35.11%\n",
      "[150/200] lr 0.01464 | train 72.52% | val 36.22%\n",
      "[151/200] lr 0.01409 | train 70.36% | val 36.25%\n",
      "[152/200] lr 0.01355 | train 79.88% | val 35.96%\n",
      "[153/200] lr 0.01302 | train 75.12% | val 36.21%\n",
      "[154/200] lr 0.01249 | train 82.44% | val 36.58%\n",
      "[155/200] lr 0.01198 | train 80.08% | val 36.42%\n",
      "[156/200] lr 0.01147 | train 80.52% | val 35.82%\n",
      "[157/200] lr 0.01098 | train 82.84% | val 35.98%\n",
      "[158/200] lr 0.01049 | train 80.60% | val 36.56%\n",
      "[159/200] lr 0.01002 | train 82.00% | val 37.40%\n",
      "[160/200] lr 0.00955 | train 79.56% | val 37.36%\n",
      "[161/200] lr 0.00909 | train 93.28% | val 37.62%\n",
      "[162/200] lr 0.00865 | train 82.72% | val 37.18%\n",
      "[163/200] lr 0.00821 | train 91.92% | val 37.68%\n",
      "[164/200] lr 0.00778 | train 77.88% | val 37.00%\n",
      "[165/200] lr 0.00737 | train 79.36% | val 36.22%\n",
      "[166/200] lr 0.00696 | train 85.44% | val 36.65%\n",
      "[167/200] lr 0.00657 | train 77.28% | val 36.71%\n",
      "[168/200] lr 0.00618 | train 86.08% | val 37.10%\n",
      "[169/200] lr 0.00581 | train 92.08% | val 37.18%\n",
      "[170/200] lr 0.00545 | train 80.56% | val 37.55%\n",
      "[171/200] lr 0.00510 | train 86.40% | val 37.08%\n",
      "[172/200] lr 0.00476 | train 88.20% | val 37.29%\n",
      "[173/200] lr 0.00443 | train 81.08% | val 37.76%\n",
      "[174/200] lr 0.00411 | train 84.96% | val 37.75%\n",
      "[175/200] lr 0.00381 | train 83.84% | val 37.61%\n",
      "[176/200] lr 0.00351 | train 80.64% | val 37.54%\n",
      "[177/200] lr 0.00323 | train 91.24% | val 37.55%\n",
      "[178/200] lr 0.00296 | train 79.84% | val 37.93%\n",
      "[179/200] lr 0.00270 | train 82.84% | val 37.66%\n",
      "[180/200] lr 0.00245 | train 84.48% | val 37.97%\n",
      "[181/200] lr 0.00221 | train 76.56% | val 37.66%\n",
      "[182/200] lr 0.00199 | train 86.44% | val 37.72%\n",
      "[183/200] lr 0.00177 | train 79.84% | val 38.01%\n",
      "[184/200] lr 0.00157 | train 74.00% | val 37.71%\n",
      "[185/200] lr 0.00138 | train 78.40% | val 37.94%\n",
      "[186/200] lr 0.00120 | train 88.20% | val 37.77%\n",
      "[187/200] lr 0.00104 | train 80.60% | val 38.07%\n",
      "[188/200] lr 0.00089 | train 76.28% | val 37.91%\n",
      "[189/200] lr 0.00074 | train 76.88% | val 38.10%\n",
      "[190/200] lr 0.00062 | train 85.24% | val 37.89%\n",
      "[191/200] lr 0.00050 | train 80.28% | val 38.16%\n",
      "[192/200] lr 0.00039 | train 90.76% | val 38.32%\n",
      "[193/200] lr 0.00030 | train 82.60% | val 37.92%\n",
      "[194/200] lr 0.00022 | train 85.40% | val 38.14%\n",
      "[195/200] lr 0.00015 | train 68.88% | val 38.01%\n",
      "[196/200] lr 0.00010 | train 78.32% | val 37.89%\n",
      "[197/200] lr 0.00006 | train 87.56% | val 38.22%\n",
      "[198/200] lr 0.00002 | train 86.44% | val 37.89%\n",
      "[199/200] lr 0.00001 | train 85.72% | val 38.03%\n",
      "[200/200] lr 0.00000 | train 67.72% | val 37.88%\n"
     ]
    }
   ],
   "source": [
    "# ---------------- hyper-parameters ---------------------------------\n",
    "T_REFRESH    = epochs         # epochs between refreshes\n",
    "SUBSET_FRACTION = 0.05   # e.g. 5% of training set\n",
    "SKETCH_L     = 256       # FD width\n",
    "BATCH_DATA   = 200\n",
    "CHUNK_GRAD   = 32\n",
    "NUM_EPOCHS   = epochs\n",
    "\n",
    "# New parameters:\n",
    "WARMUP_EPOCHS         = 0      # Set >0 for warmup, 0 for immediate selection\n",
    "FIRST_SELECTION_ON_INIT = True # If True, select at epoch 1, else after warmup\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "full_loader  = DataLoader(train_dataset, batch_size=BATCH_DATA, shuffle=True)\n",
    "train_loader = full_loader                       # start with full data\n",
    "val_loader   = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "if WARMUP_EPOCHS > 0:\n",
    "    first_selection_epoch = WARMUP_EPOCHS + 1\n",
    "elif FIRST_SELECTION_ON_INIT:\n",
    "    first_selection_epoch = 1\n",
    "else:\n",
    "    raise ValueError(\"Either WARMUP_EPOCHS > 0 or FIRST_SELECTION_ON_INIT must be True\")\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "\n",
    "    # --- Single, clear selection logic ---\n",
    "    do_selection = False\n",
    "    if epoch == first_selection_epoch:\n",
    "        if WARMUP_EPOCHS > 0:\n",
    "            print(f\"\\n⇢ Finished {WARMUP_EPOCHS} warmup epochs. Selecting subset at epoch {epoch}\")\n",
    "        else:\n",
    "            print(f\"\\n⇢ Selecting subset at epoch {epoch}\")\n",
    "        do_selection = True\n",
    "    elif epoch > first_selection_epoch and (epoch - first_selection_epoch) % T_REFRESH == 0:\n",
    "        print(f\"\\n⇢ Refreshing subset at epoch {epoch}\")\n",
    "        do_selection = True\n",
    "\n",
    "    if do_selection:\n",
    "        B_sketch = build_sketch(model_for_grad, full_loader,\n",
    "                                proj_matrix=None, d=SKETCH_L,\n",
    "                                batch_size_fd=BATCH_DATA, device=device)\n",
    "        proj_matrix = B_sketch.to(device, torch.float16)\n",
    "        k_per_cls = int(SUBSET_FRACTION * len(train_dataset) / num_classes)\n",
    "        new_ids = class_balanced_agreeing_subset_fast(\n",
    "            model_for_grad, train_dataset,\n",
    "            num_classes, k_per_cls,\n",
    "            criterion, device, proj_matrix,\n",
    "            batch_size_data=BATCH_DATA, chunk_size_grad=CHUNK_GRAD\n",
    "        )\n",
    "        subset_ds   = Subset(train_dataset, new_ids)\n",
    "        train_loader = DataLoader(subset_ds, batch_size=BATCH_DATA, shuffle=True)\n",
    "        print(f\"   new subset size: {len(subset_ds)}\")\n",
    "    # ---- training ----\n",
    "    # train_one_epoch(model, train_loader)\n",
    "    # val_loss, val_acc = evaluate(model, val_loader)\n",
    "    # print(f\"[{epoch}] val-acc={val_acc:5.2%}\")\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader)\n",
    "    val_loss,   val_acc   = evaluate(model, val_loader)\n",
    "\n",
    "    scheduler.step()            # cosine annealing step\n",
    "\n",
    "    lr_now = scheduler.get_last_lr()[0]\n",
    "    print(f\"[{epoch:3d}/{epochs}] lr {lr_now:7.5f} | \"\n",
    "          f\"train {train_acc*100:5.2f}% | val {val_acc*100:5.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8 (jupyter38-custom)",
   "language": "python",
   "name": "jupyter38custom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
